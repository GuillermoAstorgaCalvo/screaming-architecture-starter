# robots.txt - Production Best Practices
# For more information: https://www.robotstxt.org/

# Allow all search engines to crawl the site
User-agent: *
Allow: /

# Disallow sensitive and non-public paths
Disallow: /api/
Disallow: /admin/
Disallow: /auth/
Disallow: /login/
Disallow: /private/
Disallow: /internal/
Disallow: /*?*  # Disallow URLs with query parameters to avoid duplicate content

# Disallow common temporary and system paths
Disallow: /.env
Disallow: /.git/
Disallow: /node_modules/
Disallow: /dist/
Disallow: /build/

# Crawl-delay: Be polite to servers (in seconds)
# Adjust based on your server capacity and crawl patterns
# Default: 1 second (many bots ignore this, but good practice)
Crawl-delay: 1

# Sitemap location (update with your actual domain)
# Uncomment and update when sitemap is available
# Sitemap: https://example.com/sitemap.xml

# Additional rules for specific bots (optional)
# Uncomment and customize as needed

# Googlebot-specific rules (usually follows general rules, but can be customized)
# User-agent: Googlebot
# Allow: /
# Disallow: /api/
# Disallow: /admin/

# Block aggressive bots (optional - adjust as needed)
# User-agent: AhrefsBot
# Crawl-delay: 10
# User-agent: SemrushBot
# Crawl-delay: 10
# User-agent: DotBot
# Crawl-delay: 10
